{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structures for computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 281 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph: topological_sort and visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topological_sort (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "function reset_gradients!(graph)\n",
    "    for node in graph\n",
    "        node.gradient = nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScalarOperator logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 11 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: ^ , *, +, -, /\n",
    "\n",
    "^(x::GraphNode, n::GraphNode) = ScalarOperator(^, x, n)\n",
    "forward(::ScalarOperator{typeof(^)}, x, n) = return x^n\n",
    "backward(::ScalarOperator{typeof(^)}, x, n, g) = tuple(g * n * x ^ (n-1), g * log(abs(x)) * x ^ n)\n",
    "\n",
    "+(x::GraphNode, y::GraphNode) = ScalarOperator(+, x, y)\n",
    "forward(::ScalarOperator{typeof(+)}, x, y) = x + y\n",
    "backward(::ScalarOperator{typeof(+)}, x, y, gradient) = (gradient, gradient)\n",
    "\n",
    "-(x::GraphNode, y::GraphNode) = ScalarOperator(-, x, y)\n",
    "forward(::ScalarOperator{typeof(-)}, x, y) = x - y\n",
    "backward(::ScalarOperator{typeof(-)}, x, y, gradient) = (gradient, -gradient)\n",
    "\n",
    "*(x::GraphNode, y::GraphNode) = ScalarOperator(*, x, y)\n",
    "forward(::ScalarOperator{typeof(*)}, x, y) = x * y\n",
    "backward(::ScalarOperator{typeof(*)}, x, y, gradient) = (y' * gradient, x' * gradient)\n",
    "\n",
    "/(x::GraphNode, y::GraphNode) = ScalarOperator(/, x, y)\n",
    "forward(::ScalarOperator{typeof(/)}, x, y) = x / y\n",
    "backward(::ScalarOperator{typeof(/)}, x, y, gradient) = (gradient / y, gradient / y)\n",
    "\n",
    "\n",
    "import Base: sin , max, min, log\n",
    "sin(x::GraphNode) = ScalarOperator(sin, x)\n",
    "forward(::ScalarOperator{typeof(sin)}, x) = return sin(x)\n",
    "backward(::ScalarOperator{typeof(sin)}, x, g) = tuple(g * cos(x))\n",
    "\n",
    "log(x::GraphNode) = ScalarOperator(log, x)\n",
    "forward(::ScalarOperator{typeof(log)}, x) = log(x)\n",
    "backward(::ScalarOperator{typeof(log)}, x, gradient) = (gradient / x)\n",
    "\n",
    "max(x::GraphNode, y::GraphNode) = ScalarOperator(max, x, y)\n",
    "forward(::ScalarOperator{typeof(max)}, x, y) = max(x, y)\n",
    "backward(::ScalarOperator{typeof(max)}, x, y, gradient) = (gradient * isless(y, x), gradient * isless(x, y))\n",
    "\n",
    "min(x::GraphNode, y::GraphNode) = ScalarOperator(min, x, y)\n",
    "forward(::ScalarOperator{typeof(min)}, x, y) = min(x, y)\n",
    "backward(::ScalarOperator{typeof(min)}, x, y, gradient) = (gradient * isless(x, y), gradient * isless(y, x))\n",
    "\n",
    "relu(x::GraphNode) = ScalarOperator(relu, x)\n",
    "forward(::ScalarOperator{typeof(relu)}, x) = max(x, 0)\n",
    "backward(::ScalarOperator{typeof(relu)}, x, gradient) = gradient * isless(0, x)\n",
    "\n",
    "logistic(x::GraphNode) = ScalarOperator(logistic, x)\n",
    "forward(::ScalarOperator{typeof(logistic)}, x) = 1 / (1 + exp(-x))\n",
    "backward(::ScalarOperator{typeof(logistic)}, x, gradient) = gradient * exp(-x) / (1 + exp(-x))^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast Operator logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 19 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: *\n",
    "import LinearAlgebra: mul!, diagm\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# x .* y (element-wise multiplication)\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(y .* 𝟏)\n",
    "    Jy = diagm(x .* 𝟏)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g)\n",
    "\n",
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    𝟏 = ones(length(x))\n",
    "    J = 𝟏'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y::Real, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(𝟏 ./ y)\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: tanh\n",
    "Base.Broadcast.broadcasted(tanh, x::GraphNode)= BroadcastedOperator(tanh,x; name=\"tanh\")\n",
    "forward(::BroadcastedOperator{typeof(tanh)},x) = return tanh.(x)\n",
    "backward(::BroadcastedOperator{typeof(tanh)},x, g) = let \n",
    "    𝟙 = ones(length(node.output))\n",
    "    tuple((𝟙 - tanh.(x).^2) * g)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN - tanh and dense implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 21 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_cell(wx::GraphNode, wh::GraphNode, b::GraphNode, x, h::GraphNode) = BroadcastedOperator(RNN_cell, wx, wh, b, x, h; name=\"RNN_cell\")\n",
    "forward(::BroadcastedOperator{typeof(RNN_cell)}, wx, wh, b, x ,h) = let \n",
    "    tmp = wx * x .+ wh * h .+ b\n",
    "    return tanh.(tmp)\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(RNN_cell)}, wx, wh, b, x, h, g) = let \n",
    "    tmp = wx * x .+ wh * h .+ b\n",
    "    dtanh = 1 .- tanh.(tmp).^2\n",
    "    g = g .* dtanh\n",
    "    tuple(g * x', g * h', sum(g,dims=2), wx' *g, wh' * g)\n",
    "end\n",
    "\n",
    "dense(w::GraphNode, b::GraphNode, x::GraphNode) = BroadcastedOperator(dense, w , b, x; name=\"dense\")\n",
    "forward(::BroadcastedOperator{typeof(dense)}, w, b , x) = let \n",
    "    w * x .+ b\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(dense)}, w, b, x, g) = let \n",
    "    tuple(g * x', sum(g,dims=2), w' * g)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 22 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(yhat::GraphNode, y::GraphNode) = BroadcastedOperator(cross_entropy_loss, yhat, y; name=\"cross_entropy_loss\")\n",
    "forward(::BroadcastedOperator{typeof(cross_entropy_loss)}, ŷ, y) =\n",
    "let\n",
    "    eps = 1e-8\n",
    "    ŷ = ŷ .- maximum(ŷ; dims=1)\n",
    "    softmax = exp.(ŷ) ./ sum(exp.(ŷ); dims=1)\n",
    "    softmax = clamp.(softmax, eps, 1.0 - eps)\n",
    "    loss = -sum(y .* log.(softmax .+eps); dims=1) \n",
    "    return mean(loss)\n",
    "end\n",
    "backward(node::BroadcastedOperator{typeof(cross_entropy_loss)}, ŷ, y, g) =\n",
    "let\n",
    "    eps = 1e-8\n",
    "    ŷ = ŷ .- maximum(ŷ; dims=1)  # for numerical stability\n",
    "    softmax = exp.(ŷ) ./ sum(exp.(ŷ); dims=1)\n",
    "    softmax = clamp.(softmax, eps, 1.0 - eps)\n",
    "    grad_yhat = softmax - y\n",
    "    return (g .* grad_yhat,)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Statistics: mean  \n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data = MLDatasets.MNIST(split=:test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onecold (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "function loader(data; batchsize::Int=1, shuffle::Bool=true)\n",
    "    x1dim = reshape(data.features, 28 * 28, :) # reshape 28×28 pixels into a vector of pixels\n",
    "    yhot = onehotbatch(data.targets, 0:9) # make a 10×60000 one-hot matrix\n",
    "    \n",
    "    dataset = (x1dim, yhot)\n",
    "    \n",
    "    num_samples = size(x1dim, 2)\n",
    "    \n",
    "    function create_batches()\n",
    "        indices = shuffle ? Random.shuffle(1:num_samples) : 1:num_samples\n",
    "        batches = []\n",
    "        \n",
    "        for i in 1:batchsize:num_samples\n",
    "            end_idx = min(i+batchsize-1, num_samples)\n",
    "            push!(batches, (x1dim[:, indices[i:end_idx]], yhot[:, indices[i:end_idx]]))\n",
    "        end\n",
    "        \n",
    "        return batches\n",
    "    end\n",
    "    \n",
    "    return create_batches()\n",
    "end\n",
    "\n",
    "\n",
    "function onehotbatch(targets, classes)\n",
    "    onehot = zeros(Int, length(classes), length(targets))\n",
    "    for (i, target) in enumerate(targets)\n",
    "        onehot[target+1, i] = 1\n",
    "    end\n",
    "    return onehot\n",
    "end\n",
    "\n",
    "function onecold(y)\n",
    "    return argmax(y, dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Recurent_stage(Wx::GraphNode, Wh::GraphNode, b1::GraphNode, x, h)\n",
    "    x̂ = RNN_cell(Wx, Wh, b1, x, h)\n",
    "    x̂.name = \"x̂\"\n",
    "    x̂\n",
    "end\n",
    "\n",
    "function dense(w, b, x)\n",
    "    w * x .+ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net_test (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function net_test(data, Wx, Wh, W, b1, b2)\n",
    "    (x,y) = loader(data; batchsize=length(data)) |> first\n",
    "    x_var = Variable(x[1:196,:], name=\"x\")\n",
    "    y = Variable(y, name=\"y\")\n",
    "    h = Recurent_stage(Wx, Wh, b1,  x_var, Variable(zeros(64,length(data))))\n",
    "    x_var = Variable(x[197:392,:], name=\"x\")\n",
    "    h =  Recurent_stage(Wx, Wh,  b1,  x_var, h)\n",
    "    x_var = Variable(x[393:588,:], name=\"x\")\n",
    "    h =  Recurent_stage(Wx, Wh,  b1,  x_var, h)\n",
    "    x_var = Variable(x[589:end,:], name=\"x\")\n",
    "    x̂ =  Recurent_stage(Wx, Wh,  b1, x_var, h)\n",
    "    ŷ = dense(W, b2, x̂)\n",
    "    E = cross_entropy_loss(ŷ, y)\n",
    "    E.name = \"loss\"\n",
    "    E.inputs\n",
    "    graph = topological_sort(E)\n",
    "    loss = forward!(graph)\n",
    "    acc = round(100*mean(onecold(ŷ.output) .== onecold(y.output)); digits=2)\n",
    "    (; loss, acc, split=data.split)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights update and clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cliping (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function weights_update(graph::Vector, lr=0.0001)\n",
    "    for node in graph\n",
    "        if isa(node, Variable) && !isnothing(node.gradient)\n",
    "            node.output .-= lr*node.gradient\n",
    "            node.gradient .= 0.0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function cliping(graph::Vector)\n",
    "    clip_value = 5.0 #5.0 best val\n",
    "    # After backward pass, before weights update\n",
    "    for node in graph\n",
    "        if isa(node, Variable) && !isnothing(node.gradient)\n",
    "            \n",
    "            node.gradient .= clamp.(node.gradient, -clip_value, clip_value)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var b2\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ Nothing"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 14*14\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "\n",
    "function xavier_init(out_size, in_size, gain)\n",
    "    return randn(out_size, in_size) .* gain * sqrt(6.0 / (in_size + out_size))\n",
    "end\n",
    "\n",
    "global Wx = Variable(xavier_init(hidden_size, input_size,2), name=\"Wx\")\n",
    "global Wh = Variable(xavier_init(hidden_size, hidden_size,2), name=\"Wh\")\n",
    "global W  = Variable(xavier_init(output_size, hidden_size,2), name=\"W\")\n",
    "global b1 = Variable(randn(hidden_size), name=\"b1\")\n",
    "global b2 = Variable(randn(output_size), name=\"b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_loop(train_data, test_data, Wx, Wh, W, b1, b2)\n",
    "    best_acc = 0.0\n",
    "    last_improvent = 0\n",
    "    lr = 1e-4\n",
    "    for epoch in 1:30\n",
    "        for (x,y) in loader(train_data; batchsize = 100)\n",
    "            h = Variable(zeros(64,100), name=\"h\")\n",
    "            x_var = Variable(x[1:196,:], name=\"x\")\n",
    "            y = Variable(y, name=\"y\")\n",
    "            h = Recurent_stage(Wx, Wh, b1,  x_var, h)\n",
    "            h.name = \"h\"\n",
    "            x_var = Variable(x[197:392,:], name=\"x\")\n",
    "            h =  Recurent_stage(Wx, Wh,  b1,  x_var, h)\n",
    "            h.name = \"h\"\n",
    "            x_var = Variable(x[393:588,:], name=\"x\")\n",
    "            h =  Recurent_stage(Wx, Wh,  b1,  x_var, h)\n",
    "            h.name = \"h\"\n",
    "            x_var = Variable(x[589:end,:], name=\"x\")\n",
    "            x̂ =  Recurent_stage(Wx, Wh,  b1, x_var, h)\n",
    "            h.name = \"x̂\"\n",
    "            ŷ = dense(W, b2, x̂)\n",
    "            E = cross_entropy_loss(ŷ, y)\n",
    "            E.name = \"loss\"\n",
    "            graph = topological_sort(E)\n",
    "            forward!(graph)\n",
    "            reset_gradients!(graph)\n",
    "            backward!(graph)\n",
    "            cliping(graph)  \n",
    "            weights_update(graph, lr)\n",
    "        end\n",
    "        loss_train, acc_train, _ = net_test(train_data, Wx, Wh, W, b1, b2)\n",
    "        loss_test, acc_test, _ = net_test(test_data, Wx, Wh, W, b1, b2)\n",
    "\n",
    "        if acc_train > best_acc\n",
    "            best_acc = acc_train\n",
    "            last_improvent = epoch\n",
    "        end\n",
    "        if  epoch - last_improvent >= 10 && lr > 1e-6\n",
    "            lr /= 10\n",
    "            last_improvent = epoch\n",
    "        end\n",
    "        if epoch - last_improvent >= 20\n",
    "            @warn \"Early stopping no inprovement in 20 epochs\"\n",
    "            break\n",
    "        end\n",
    "        @info \"Epoch: $epoch, Train Loss: $loss_train, Train Accuracy: $acc_train, Test Loss: $loss_test, Test Accuracy: $acc_test\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 1, Train Loss: 1.119195800928724, Train Accuracy: 64.17, Test Loss: 1.1210559024228104, Test Accuracy: 63.4\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 2, Train Loss: 0.8214690115313068, Train Accuracy: 73.54, Test Loss: 0.823043517992253, Test Accuracy: 73.46\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 3, Train Loss: 0.6874689489276021, Train Accuracy: 78.16, Test Loss: 0.6897551493535873, Test Accuracy: 78.02\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 4, Train Loss: 0.6048918017447653, Train Accuracy: 80.92, Test Loss: 0.6104741547752669, Test Accuracy: 80.89\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 5, Train Loss: 0.5459461338035745, Train Accuracy: 83.09, Test Loss: 0.5526280327059764, Test Accuracy: 82.92\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 6, Train Loss: 0.5029265722824178, Train Accuracy: 84.52, Test Loss: 0.5114484524965356, Test Accuracy: 84.27\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 7, Train Loss: 0.46835064916345337, Train Accuracy: 85.6, Test Loss: 0.4779442907620427, Test Accuracy: 85.45\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 8, Train Loss: 0.44012062410246156, Train Accuracy: 86.6, Test Loss: 0.4526339912226005, Test Accuracy: 86.26\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 9, Train Loss: 0.41638877018812387, Train Accuracy: 87.37, Test Loss: 0.4312798629590121, Test Accuracy: 86.94\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 10, Train Loss: 0.39754243161962927, Train Accuracy: 88.0, Test Loss: 0.4153576945906945, Test Accuracy: 87.33\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 11, Train Loss: 0.37995813960468394, Train Accuracy: 88.55, Test Loss: 0.4001031949475311, Test Accuracy: 87.94\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 12, Train Loss: 0.3642548234027138, Train Accuracy: 89.02, Test Loss: 0.3861465834614538, Test Accuracy: 88.35\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 13, Train Loss: 0.35277337956989396, Train Accuracy: 89.4, Test Loss: 0.3759640427036653, Test Accuracy: 88.56\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 14, Train Loss: 0.3398954256955441, Train Accuracy: 89.71, Test Loss: 0.366116404270145, Test Accuracy: 89.11\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 15, Train Loss: 0.32885062542442917, Train Accuracy: 90.12, Test Loss: 0.35551128652059566, Test Accuracy: 89.39\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 16, Train Loss: 0.31996997614548517, Train Accuracy: 90.35, Test Loss: 0.3471016736441671, Test Accuracy: 89.55\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 17, Train Loss: 0.3116639817492297, Train Accuracy: 90.58, Test Loss: 0.3403055080528683, Test Accuracy: 89.74\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 18, Train Loss: 0.3028101615343415, Train Accuracy: 90.85, Test Loss: 0.3330722284909332, Test Accuracy: 89.96\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 19, Train Loss: 0.2958039933510904, Train Accuracy: 91.06, Test Loss: 0.32669799069144506, Test Accuracy: 90.31\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 20, Train Loss: 0.2885536056703268, Train Accuracy: 91.25, Test Loss: 0.32098471935679107, Test Accuracy: 90.41\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 21, Train Loss: 0.2824227670955811, Train Accuracy: 91.48, Test Loss: 0.3153811493347458, Test Accuracy: 90.67\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 22, Train Loss: 0.2764205597388111, Train Accuracy: 91.67, Test Loss: 0.3107777182004842, Test Accuracy: 90.89\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 23, Train Loss: 0.27078363415642936, Train Accuracy: 91.84, Test Loss: 0.3063075031279613, Test Accuracy: 91.04\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 24, Train Loss: 0.26561264091062736, Train Accuracy: 92.0, Test Loss: 0.30222284090011, Test Accuracy: 91.2\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 25, Train Loss: 0.26069350913632927, Train Accuracy: 92.16, Test Loss: 0.2982049065087308, Test Accuracy: 91.29\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 26, Train Loss: 0.25556203045531967, Train Accuracy: 92.32, Test Loss: 0.29377277120460293, Test Accuracy: 91.44\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 27, Train Loss: 0.2513608882209106, Train Accuracy: 92.41, Test Loss: 0.29071383003853607, Test Accuracy: 91.53\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 28, Train Loss: 0.24731019566184534, Train Accuracy: 92.6, Test Loss: 0.2877970555040724, Test Accuracy: 91.64\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 29, Train Loss: 0.2436576354635473, Train Accuracy: 92.74, Test Loss: 0.2856453778095337, Test Accuracy: 91.76\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 30, Train Loss: 0.23990083853062544, Train Accuracy: 92.79, Test Loss: 0.2819958091789412, Test Accuracy: 91.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99.119460 seconds (22.00 M allocations: 157.763 GiB, 9.56% gc time, 8.25% compilation time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m169396261624\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mElapsed time: 99.1997561454773\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "@time @info @allocated train_loop(train_data, test_data, Wx, Wh, W, b1, b2)\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "@info \"Elapsed time: $elapsed_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
